{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "urj8WfyEBe3j"
      },
      "source": [
        "#IIC-2433 Minería de Datos UC\n",
        "\n",
        "* Versiones de librerías, python 3.8.10\n",
        "* pandas 1.5.3\n",
        "* tensorflow/keras 2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KpscHAvkDcmh"
      },
      "source": [
        "# 1. Carga de la base de datos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "luvxU_uaY6sL"
      },
      "source": [
        "En esta tarea se trabajará con un dataset de vinos obtenido de Kaggle:\n",
        "Cárguelo, léalo y muéstrelo.\n",
        "\n",
        "https://www.kaggle.com/datasets/yasserh/wine-quality-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y6vYAYIV4yKQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1143, 13)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dframe = pd.read_csv(\"WineQT.csv\", encoding = \"ISO-8859-1\")\n",
        "dframe.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "      <th>Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "      <td>1592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>6.8</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.068</td>\n",
              "      <td>28.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.99651</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.82</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "      <td>1593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "      <td>1594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "      <td>1595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1142</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "      <td>1597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1143 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0               7.4             0.700         0.00             1.9      0.076   \n",
              "1               7.8             0.880         0.00             2.6      0.098   \n",
              "2               7.8             0.760         0.04             2.3      0.092   \n",
              "3              11.2             0.280         0.56             1.9      0.075   \n",
              "4               7.4             0.700         0.00             1.9      0.076   \n",
              "...             ...               ...          ...             ...        ...   \n",
              "1138            6.3             0.510         0.13             2.3      0.076   \n",
              "1139            6.8             0.620         0.08             1.9      0.068   \n",
              "1140            6.2             0.600         0.08             2.0      0.090   \n",
              "1141            5.9             0.550         0.10             2.2      0.062   \n",
              "1142            5.9             0.645         0.12             2.0      0.075   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
              "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
              "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
              "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "...                   ...                   ...      ...   ...        ...   \n",
              "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
              "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
              "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
              "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
              "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
              "\n",
              "      alcohol  quality    Id  \n",
              "0         9.4        5     0  \n",
              "1         9.8        5     1  \n",
              "2         9.8        5     2  \n",
              "3         9.8        6     3  \n",
              "4         9.4        5     4  \n",
              "...       ...      ...   ...  \n",
              "1138     11.0        6  1592  \n",
              "1139      9.5        6  1593  \n",
              "1140     10.5        5  1594  \n",
              "1141     11.2        6  1595  \n",
              "1142     10.2        5  1597  \n",
              "\n",
              "[1143 rows x 13 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dframe"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dgyQ3wpeC4QS"
      },
      "source": [
        "# 2. Preprocesamiento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Cb4_ZpC7E_"
      },
      "source": [
        "Realice el preprocesamiento que considere adecuado para este *dataset* y argumente todas sus decisiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Astqo3Pb4vvZ"
      },
      "outputs": [],
      "source": [
        " #Chequeamos si hay algun valor nulo en el df, y si hay, eliminamos la fila\n",
        "if dframe.isnull().values.any():\n",
        "    dframe.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "      <th>Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "      <td>1592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>6.8</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.068</td>\n",
              "      <td>28.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.99651</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.82</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "      <td>1593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "      <td>1594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "      <td>1595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1142</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "      <td>1597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>834 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0               7.4             0.700         0.00             1.9      0.076   \n",
              "1               7.8             0.880         0.00             2.6      0.098   \n",
              "2               7.8             0.760         0.04             2.3      0.092   \n",
              "3              11.2             0.280         0.56             1.9      0.075   \n",
              "4               7.4             0.700         0.00             1.9      0.076   \n",
              "...             ...               ...          ...             ...        ...   \n",
              "1138            6.3             0.510         0.13             2.3      0.076   \n",
              "1139            6.8             0.620         0.08             1.9      0.068   \n",
              "1140            6.2             0.600         0.08             2.0      0.090   \n",
              "1141            5.9             0.550         0.10             2.2      0.062   \n",
              "1142            5.9             0.645         0.12             2.0      0.075   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
              "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
              "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
              "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "...                   ...                   ...      ...   ...        ...   \n",
              "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
              "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
              "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
              "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
              "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
              "\n",
              "      alcohol  quality    Id  \n",
              "0         9.4        5     0  \n",
              "1         9.8        5     1  \n",
              "2         9.8        5     2  \n",
              "3         9.8        6     3  \n",
              "4         9.4        5     4  \n",
              "...       ...      ...   ...  \n",
              "1138     11.0        6  1592  \n",
              "1139      9.5        6  1593  \n",
              "1140     10.5        5  1594  \n",
              "1141     11.2        6  1595  \n",
              "1142     10.2        5  1597  \n",
              "\n",
              "[834 rows x 13 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Se hace un robust scaling para eliminar outliers\n",
        "\n",
        "Q1 = dframe.quantile(0.25)\n",
        "Q3 = dframe.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "filtered_df = dframe[~((dframe < lower_bound) | (dframe > upper_bound)).any(axis=1)]\n",
        "filtered_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predeciremos Quality\n",
        "\n",
        "X = filtered_df.drop('quality', axis=1)\n",
        "y = filtered_df['quality']\n",
        "y = to_categorical(y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_7GwwQOGD3C_"
      },
      "source": [
        "# 3. Preguntas iniciales\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dS5e6dUNEi_6"
      },
      "source": [
        "Deberá encontrar la mejor combinación de hiperparámetros que le permita obtener el modelo con mejor *accuracy*.\n",
        "\n",
        "A priori, ¿cuáles cree que serán las mejores combinaciones de hiperparámetros para este problema en particular? ¿Por qué?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Un patience alto puede ayudar a obtener un mejor modelo, dado que define cuantos epochs consecutivos pueden haber sin mejora, pero si despues logra \"desestancarse\" seguira mejorando \n",
        "- un alto numero de epochs combinado con el parametro que dije antes.\n",
        "- El numero de capas neuronales para aumentar la complejidad del modelo (aunque a su vez afecta su velocidad y uso de recursos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UO8VTsHi13o9"
      },
      "source": [
        "# 3. Modelos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QsPcsGib3hxc"
      },
      "source": [
        "Ocupando la librería **keras** de tensorflow, construya redes neuronales multicapas variando los siguientes hiperparámetros: función de activación, cantidad de neuronas por cada capa, cantidad de capas, optimizador y cantidad de épocas."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### INTENTO 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_y8RXf8v4tHC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 2.1654 - accuracy: 0.0713 - val_loss: 1.9485 - val_accuracy: 0.2090\n",
            "Epoch 2/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.7957 - accuracy: 0.4165 - val_loss: 1.6203 - val_accuracy: 0.5000\n",
            "Epoch 3/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.4612 - accuracy: 0.5572 - val_loss: 1.2757 - val_accuracy: 0.5373\n",
            "Epoch 4/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.1581 - accuracy: 0.5985 - val_loss: 1.0418 - val_accuracy: 0.6045\n",
            "Epoch 5/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.9803 - accuracy: 0.6154 - val_loss: 0.9456 - val_accuracy: 0.6194\n",
            "Epoch 6/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.9030 - accuracy: 0.6417 - val_loss: 0.9130 - val_accuracy: 0.6119\n",
            "Epoch 7/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.8672 - accuracy: 0.6454 - val_loss: 0.9034 - val_accuracy: 0.6343\n",
            "Epoch 8/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.8432 - accuracy: 0.6473 - val_loss: 0.8942 - val_accuracy: 0.6119\n",
            "Epoch 9/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.8268 - accuracy: 0.6567 - val_loss: 0.8910 - val_accuracy: 0.6045\n",
            "Epoch 10/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.8107 - accuracy: 0.6548 - val_loss: 0.8872 - val_accuracy: 0.6119\n",
            "Epoch 11/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.8006 - accuracy: 0.6567 - val_loss: 0.8889 - val_accuracy: 0.6045\n",
            "Epoch 12/300\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.7907 - accuracy: 0.6623 - val_loss: 0.8844 - val_accuracy: 0.5821\n",
            "Epoch 13/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7766 - accuracy: 0.6660 - val_loss: 0.8845 - val_accuracy: 0.6119\n",
            "Epoch 14/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7723 - accuracy: 0.6604 - val_loss: 0.8843 - val_accuracy: 0.5970\n",
            "Epoch 15/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7581 - accuracy: 0.6754 - val_loss: 0.8861 - val_accuracy: 0.5821\n",
            "Epoch 16/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7513 - accuracy: 0.6642 - val_loss: 0.8826 - val_accuracy: 0.6119\n",
            "Epoch 17/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7438 - accuracy: 0.6848 - val_loss: 0.8845 - val_accuracy: 0.6045\n",
            "Epoch 18/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7331 - accuracy: 0.6829 - val_loss: 0.8834 - val_accuracy: 0.6045\n",
            "Epoch 19/300\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.7242 - accuracy: 0.6961 - val_loss: 0.8837 - val_accuracy: 0.5970\n",
            "21/21 [==============================] - 0s 700us/step\n",
            "6/6 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=300, callbacks=callbacks)\n",
        "\n",
        "y_train_pred = np.argmax(model.predict(X_train), axis=-1)\n",
        "y_test_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "y_train_label = np.argmax(y_train, axis=-1)\n",
        "y_test_label = np.argmax(y_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.6776611694152923\n",
            "\n",
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           4       0.00      0.00      0.00        17\n",
            "           5       0.68      0.80      0.74       297\n",
            "           6       0.67      0.62      0.64       280\n",
            "           7       0.69      0.56      0.62        73\n",
            "\n",
            "    accuracy                           0.68       667\n",
            "   macro avg       0.51      0.50      0.50       667\n",
            "weighted avg       0.66      0.68      0.67       667\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Accuracy: \", accuracy_score(y_train_label, y_train_pred))\n",
        "\n",
        "print(\"\\nTrain Classification Report:\")\n",
        "print(classification_report(y_train_label, y_train_pred,zero_division=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo logre altiro xd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### INTENTO 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "15/15 [==============================] - 1s 12ms/step - loss: 1.9869 - accuracy: 0.2575 - val_loss: 1.7848 - val_accuracy: 0.4188\n",
            "Epoch 2/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.5556 - accuracy: 0.4549 - val_loss: 1.3433 - val_accuracy: 0.4188\n",
            "Epoch 3/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.1921 - accuracy: 0.5215 - val_loss: 1.0505 - val_accuracy: 0.5641\n",
            "Epoch 4/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.0139 - accuracy: 0.6094 - val_loss: 0.9726 - val_accuracy: 0.5812\n",
            "Epoch 5/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.9535 - accuracy: 0.6116 - val_loss: 0.9659 - val_accuracy: 0.5726\n",
            "Epoch 6/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.9081 - accuracy: 0.6288 - val_loss: 0.9467 - val_accuracy: 0.5812\n",
            "Epoch 7/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.8586 - accuracy: 0.6373 - val_loss: 0.9269 - val_accuracy: 0.5812\n",
            "Epoch 8/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.8200 - accuracy: 0.6416 - val_loss: 0.9111 - val_accuracy: 0.6068\n",
            "Epoch 9/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7944 - accuracy: 0.6588 - val_loss: 0.8902 - val_accuracy: 0.6239\n",
            "Epoch 10/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7762 - accuracy: 0.6459 - val_loss: 0.9457 - val_accuracy: 0.5897\n",
            "Epoch 11/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7681 - accuracy: 0.6652 - val_loss: 0.8726 - val_accuracy: 0.6325\n",
            "Epoch 12/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.7266 - accuracy: 0.6674 - val_loss: 0.8664 - val_accuracy: 0.6325\n",
            "Epoch 13/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7081 - accuracy: 0.7017 - val_loss: 0.8803 - val_accuracy: 0.5897\n",
            "Epoch 14/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.7232 - val_loss: 0.8650 - val_accuracy: 0.6068\n",
            "Epoch 15/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6702 - accuracy: 0.7060 - val_loss: 0.8819 - val_accuracy: 0.6154\n",
            "Epoch 16/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6564 - accuracy: 0.7124 - val_loss: 0.8687 - val_accuracy: 0.6325\n",
            "Epoch 17/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.7318 - val_loss: 0.9352 - val_accuracy: 0.5983\n",
            "Epoch 18/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6355 - accuracy: 0.7318 - val_loss: 0.8713 - val_accuracy: 0.6325\n",
            "Epoch 19/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 0.7554 - val_loss: 0.8852 - val_accuracy: 0.6239\n",
            "Epoch 20/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.7446 - val_loss: 0.9575 - val_accuracy: 0.5726\n",
            "Epoch 21/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5780 - accuracy: 0.7725 - val_loss: 0.9102 - val_accuracy: 0.6154\n",
            "Epoch 22/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5563 - accuracy: 0.7983 - val_loss: 0.9750 - val_accuracy: 0.6239\n",
            "Epoch 23/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7790 - val_loss: 0.9100 - val_accuracy: 0.6581\n",
            "Epoch 24/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5216 - accuracy: 0.7940 - val_loss: 0.9252 - val_accuracy: 0.6325\n",
            "19/19 [==============================] - 0s 611us/step\n",
            "8/8 [==============================] - 0s 1000us/step\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=200, callbacks=callbacks)\n",
        "\n",
        "y_train_pred = np.argmax(model.predict(X_train), axis=-1)\n",
        "y_test_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "y_train_label = np.argmax(y_train, axis=-1)\n",
        "y_test_label = np.argmax(y_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.7787307032590052\n",
            "\n",
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           4       0.00      0.00      0.00        14\n",
            "           5       0.78      0.83      0.81       261\n",
            "           6       0.75      0.81      0.78       241\n",
            "           7       0.93      0.63      0.75        67\n",
            "\n",
            "    accuracy                           0.78       583\n",
            "   macro avg       0.62      0.57      0.58       583\n",
            "weighted avg       0.77      0.78      0.77       583\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Accuracy: \", accuracy_score(y_train_label, y_train_pred))\n",
        "\n",
        "print(\"\\nTrain Classification Report:\")\n",
        "print(classification_report(y_train_label, y_train_pred,zero_division=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### INTENTO 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "15/15 [==============================] - 1s 10ms/step - loss: 2.0439 - accuracy: 0.1545 - val_loss: 1.9262 - val_accuracy: 0.3419\n",
            "Epoch 2/40\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.8204 - accuracy: 0.4657 - val_loss: 1.7165 - val_accuracy: 0.4615\n",
            "Epoch 3/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.5825 - accuracy: 0.5064 - val_loss: 1.4804 - val_accuracy: 0.4786\n",
            "Epoch 4/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.3439 - accuracy: 0.5086 - val_loss: 1.2661 - val_accuracy: 0.4872\n",
            "Epoch 5/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.1686 - accuracy: 0.5815 - val_loss: 1.1178 - val_accuracy: 0.5726\n",
            "Epoch 6/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.0562 - accuracy: 0.5966 - val_loss: 1.0407 - val_accuracy: 0.5726\n",
            "Epoch 7/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.9838 - accuracy: 0.6073 - val_loss: 0.9957 - val_accuracy: 0.5726\n",
            "Epoch 8/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.9321 - accuracy: 0.6159 - val_loss: 0.9668 - val_accuracy: 0.5812\n",
            "Epoch 9/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.8906 - accuracy: 0.6180 - val_loss: 0.9407 - val_accuracy: 0.6068\n",
            "Epoch 10/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.8622 - accuracy: 0.6373 - val_loss: 0.9350 - val_accuracy: 0.6068\n",
            "Epoch 11/40\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.8342 - accuracy: 0.6631 - val_loss: 0.9031 - val_accuracy: 0.5897\n",
            "Epoch 12/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.8123 - accuracy: 0.6609 - val_loss: 0.8938 - val_accuracy: 0.5897\n",
            "Epoch 13/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7932 - accuracy: 0.6609 - val_loss: 0.8944 - val_accuracy: 0.5983\n",
            "Epoch 14/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7866 - accuracy: 0.6695 - val_loss: 0.8826 - val_accuracy: 0.5897\n",
            "Epoch 15/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7732 - accuracy: 0.6652 - val_loss: 0.8804 - val_accuracy: 0.5897\n",
            "Epoch 16/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7569 - accuracy: 0.6803 - val_loss: 0.8683 - val_accuracy: 0.5897\n",
            "Epoch 17/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7465 - accuracy: 0.6760 - val_loss: 0.8714 - val_accuracy: 0.6068\n",
            "Epoch 18/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7312 - accuracy: 0.6931 - val_loss: 0.8694 - val_accuracy: 0.6239\n",
            "Epoch 19/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7236 - accuracy: 0.6953 - val_loss: 0.8731 - val_accuracy: 0.6068\n",
            "Epoch 20/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7137 - accuracy: 0.7060 - val_loss: 0.8649 - val_accuracy: 0.6239\n",
            "Epoch 21/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.7062 - accuracy: 0.7039 - val_loss: 0.8577 - val_accuracy: 0.6154\n",
            "Epoch 22/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.7124 - val_loss: 0.8601 - val_accuracy: 0.6154\n",
            "Epoch 23/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.7082 - val_loss: 0.8524 - val_accuracy: 0.6154\n",
            "Epoch 24/40\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6727 - accuracy: 0.7253 - val_loss: 0.8570 - val_accuracy: 0.6068\n",
            "Epoch 25/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6630 - accuracy: 0.7275 - val_loss: 0.8587 - val_accuracy: 0.6154\n",
            "Epoch 26/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6543 - accuracy: 0.7361 - val_loss: 0.8591 - val_accuracy: 0.6154\n",
            "Epoch 27/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6467 - accuracy: 0.7146 - val_loss: 0.8535 - val_accuracy: 0.6239\n",
            "Epoch 28/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6367 - accuracy: 0.7382 - val_loss: 0.8624 - val_accuracy: 0.6239\n",
            "Epoch 29/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.7403 - val_loss: 0.8550 - val_accuracy: 0.6325\n",
            "Epoch 30/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.7382 - val_loss: 0.8622 - val_accuracy: 0.6325\n",
            "Epoch 31/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.7446 - val_loss: 0.8645 - val_accuracy: 0.6410\n",
            "Epoch 32/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.7618 - val_loss: 0.8556 - val_accuracy: 0.6410\n",
            "Epoch 33/40\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.7618 - val_loss: 0.8868 - val_accuracy: 0.6410\n",
            "19/19 [==============================] - 0s 555us/step\n",
            "8/8 [==============================] - 0s 857us/step\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=40, callbacks=callbacks)\n",
        "\n",
        "y_train_pred = np.argmax(model.predict(X_train), axis=-1)\n",
        "y_test_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "y_train_label = np.argmax(y_train, axis=-1)\n",
        "y_test_label = np.argmax(y_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy:  0.7272727272727273\n",
            "\n",
            "Train Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           4       0.00      0.00      0.00        14\n",
            "           5       0.69      0.89      0.78       261\n",
            "           6       0.80      0.59      0.68       241\n",
            "           7       0.71      0.73      0.72        67\n",
            "\n",
            "    accuracy                           0.73       583\n",
            "   macro avg       0.55      0.55      0.55       583\n",
            "weighted avg       0.72      0.73      0.71       583\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Accuracy: \", accuracy_score(y_train_label, y_train_pred))\n",
        "\n",
        "print(\"\\nTrain Classification Report:\")\n",
        "print(classification_report(y_train_label, y_train_pred,zero_division=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intente encontrar un modelo que obtenga un 66.6% de *accuracy*. ¿Lo logró? ¿por qué?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se logro esta mision gracias a los hiperparametros que mencionamos en el principio, en donde si tenemos una gran cantidad de epochs combinados con un buen patiente, ademas de tener un buen numero de redes neuronales, pude obtener un modelo casi con un accuracy casi 10% mayor a lo que me pedian."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hSvcEphO4ccy"
      },
      "source": [
        "Una vez obtenido el mejor clasificador, ¿cuáles son sus errores más comunes? ¿cómo se podrían mejorar?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los errores mas comunes radican en varias cosas, las mas comunes son errores dentro del dataset, en donde los datos que se tomaron para el entrenamiento no representa de manera fidegnina a la real. Tambien podemos contar con que se entreno con muchos outliers y aprendio de manera erronea como predecir los datos. Por otra parte, tambien podemos tener errores al crear el modelo, usando hiperparametros que no favorezcan al modelo. Todos estos problemas se pueden mejorar ya sea preprocesando el dataset y haciendo una busqueda de grilla para obtener los mejores parametros del modelo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MvdM6uNb1-Gs"
      },
      "source": [
        "# 4. Preguntas finales"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlNhCmU32e5t"
      },
      "source": [
        "¿Qué resultados obtuvo? Se habrá dado cuenta que encontrar el mejor modelo no es lo mismo que buscar los mejores hiperparámetros unilateralmente. ¿Cuáles combinaciones funcionaron mejor y cuáles peor? ¿Por qué cree que fue así? Argumente."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mi mejor resultado, al menos a la hora que se corrio el codigo con esta respuesta, es de un accuracy de 80%, la combinacion que se uso para lograr esto es un patiente de 10 con un red de 128, 64, 32 y 16 neuronas. en este intento la cantidad de epoch no fue necesaria que fuera alta, dado que el patiente lo detuvo en el intento 24. La combinacion que peor funciono fue cuando se usaron menos redes neuronales, dando resultados con un accuracy menor al 70%\n",
        "La razon por la que el accuracy aumentan proporcionalmente con las redes neuronales es porque esta le permite aumentar la complejidad del modelo, por lo que si el dataset, como en este caso, tiene un buen procesamiento, puede ser de mucha ayuda a la hora de mejorar las predicciones."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uUosnZZmhT2i"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
